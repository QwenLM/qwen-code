# Ollama å¿«é€Ÿå¼€å§‹æŒ‡å—

## 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### 1ï¸âƒ£ å®‰è£… Ollama
```bash
# macOS (ä½¿ç”¨ Homebrew)
brew install ollama

# æˆ–è®¿é—® https://ollama.ai ä¸‹è½½
```

### 2ï¸âƒ£ å¯åŠ¨ Ollama
```bash
ollama serve
```
ä¿æŒç»ˆç«¯è¿è¡Œï¼ŒOllama é»˜è®¤åœ¨ `http://localhost:11434` ä¸Šè¿è¡Œ

### 3ï¸âƒ£ æ‹‰å–æ¨¡åž‹ï¼ˆåœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼‰
```bash
# æ‹‰å–ä¸€ä¸ªè½»é‡çº§æ¨¡åž‹ï¼ˆæŽ¨èï¼‰
ollama pull mistral

# æˆ–å…¶ä»–é€‰æ‹©
ollama pull llama2
ollama pull neural-chat
```

### 4ï¸âƒ£ é…ç½® Qwen Code

ç¼–è¾‘æˆ–åˆ›å»º `~/.qwen/settings.json`ï¼š

```bash
mkdir -p ~/.qwen
cat > ~/.qwen/settings.json << 'EOF'
{
  "modelProviders": {
    "ollama": [
      {
        "id": "mistral",
        "name": "Mistral 7B",
        "description": "Fast and efficient 7B model",
        "baseUrl": "http://localhost:11434"
      },
      {
        "id": "llama2",
        "name": "Llama 2",
        "description": "Full-featured model",
        "baseUrl": "http://localhost:11434"
      }
    ]
  }
}
EOF
```

### 5ï¸âƒ£ å¯åŠ¨ Qwen Code
```bash
cd /path/to/qwen-code
npm run start
```

### 6ï¸âƒ£ é€‰æ‹©æ¨¡åž‹
åœ¨ Qwen Code ä¸­è¾“å…¥ï¼š
```
/model
```

çŽ°åœ¨æ‚¨åº”è¯¥çœ‹åˆ° Ollama æ¨¡åž‹åˆ—è¡¨ï¼ ðŸŽ‰

## å°±è¿™ä¹ˆç®€å•ï¼

æ‚¨çŽ°åœ¨å¯ä»¥ï¼š
- ðŸš€ ä½¿ç”¨æœ¬åœ° LLM æ¨¡åž‹
- ðŸ”’ ä¿æŠ¤æ‚¨çš„æ•°æ®éšç§
- ðŸŒ ç¦»çº¿å·¥ä½œ
- âš¡ è‡ªå®šä¹‰æ¨¡åž‹é…ç½®

## éœ€è¦å¸®åŠ©ï¼Ÿ

æŸ¥çœ‹å®Œæ•´æŒ‡å—ï¼š`docs/users/integration-ollama.md`

æˆ–æŸ¥çœ‹å¸¸è§é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼š`OLLAMA_CONFIG_EXAMPLE.md`
